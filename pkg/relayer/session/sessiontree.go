package session

import (
	"bytes"
	"fmt"
	"os"
	"path/filepath"
	"sync"

	"github.com/pokt-network/smt"
	"github.com/pokt-network/smt/kvstore"
	"github.com/pokt-network/smt/kvstore/pebble"
	"github.com/pokt-network/smt/kvstore/simplemap"

	"github.com/pokt-network/poktroll/pkg/crypto/protocol"
	"github.com/pokt-network/poktroll/pkg/polylog"
	"github.com/pokt-network/poktroll/pkg/relayer"
	prooftypes "github.com/pokt-network/poktroll/x/proof/types"
	sessiontypes "github.com/pokt-network/poktroll/x/session/types"
)

var _ relayer.SessionTree = (*sessionTree)(nil)

// sessionTree is an implementation of the SessionTree interface.
type sessionTree struct {
	logger polylog.Logger

	// sessionMu is a mutex used to protect sessionTree operations from concurrent access.
	sessionMu *sync.Mutex

	// sessionHeader is the header of the session corresponding to the SMST (Sparse Merkle State Trie).
	sessionHeader *sessiontypes.SessionHeader

	// sessionSMT is the SMST (Sparse Merkle State Trie) corresponding the session.
	sessionSMT smt.SparseMerkleSumTrie

	// supplierOperatorAddress is the address of the supplier's operator that owns this sessionTree.
	// RelayMiner can run suppliers for many supplier operator addresses at the same time,
	// and we need a way to group the session trees by the supplier operator address for that.
	supplierOperatorAddress string

	// claimedRoot is the root hash of the SMST needed for submitting the claim.
	// If it holds a non-nil value, it means that the SMST has been flushed,
	// committed to disk and no more updates can be made to it. A non-nil value also
	// indicates that a proof could be generated using ProveClosest function.
	claimedRoot []byte

	// proofPath is the path for which the proof was generated.
	proofPath []byte

	// compactProof is the generated compactProof for the session given a proofPath.
	compactProof *smt.SparseCompactMerkleClosestProof

	// compactProofBz is the marshaled proof for the session.
	compactProofBz []byte

	// treeStore is the KVStore used to store the SMST.
	// This can be either an in-memory store or a disk-based store.
	treeStore kvstore.MapStore

	// storePath is the path to the KVStore used to store the SMST.
	// It is created from the storePrefix and the session.sessionId.
	// We keep track of it so we can use it at the end of the claim/proof lifecycle
	// to delete the KVStore when it is no longer needed.
	storePath string

	isClaiming bool
}

// NewSessionTree creates a new sessionTree from a Session and a storePrefix. It also takes a function
// removeFromRelayerSessions that removes the sessionTree from the RelayerSessionsManager.
// It returns an error if the KVStore fails to be created.
func NewSessionTree(
	logger polylog.Logger,
	sessionHeader *sessiontypes.SessionHeader,
	supplierOperatorAddress string,
	storesDirectoryPath string,
	backupConfig *BackupConfig, // Optional backup configuration
) (relayer.SessionTree, error) {
	logger = logger.With(
		"session_id", sessionHeader.SessionId,
		"application_address", sessionHeader.ApplicationAddress,
		"service_id", sessionHeader.ServiceId,
		"supplier_operator_address", supplierOperatorAddress,
	)

	// TODO_IMPROVE(#621): Use single KV store per supplier with prefixed keys instead of one per session.
	// Initialize the KVStore based on the storage type specified
	storePath := storesDirectoryPath
	var treeStore kvstore.MapStore
	switch storesDirectoryPath {
	case "":
		return nil, fmt.Errorf("invalid storage path: empty string is not supported for disk storage")

	case InMemoryStoreFilename:
		// SimpleMap in-memory storage (pure Go map)
		primaryStore := simplemap.NewSimpleMap()

		// Check if backup is enabled
		if backupConfig != nil && backupConfig.Enabled {
			// Create backup store with throttling
			backupPath := backupConfig.GetBackupPath(supplierOperatorAddress, sessionHeader.SessionId)

			// Ensure backup directory exists
			backupDir := filepath.Dir(backupPath)
			if err := os.MkdirAll(backupDir, 0755); err != nil {
				return nil, fmt.Errorf("failed to create backup directory: %w", err)
			}

			// Create Pebble store for backup (direct writes, no throttling)
			backupStore, err := pebble.NewKVStore(backupPath)
			if err != nil {
				return nil, fmt.Errorf("failed to create backup store: %w", err)
			}

			// Create the backup wrapper with direct writes to backup store
			treeStore = NewBackupKVStore(logger, primaryStore, backupStore)
			logger.Info().
				Str("backup_path", backupPath).
				Msg("Using SimpleMap in-memory store with disk backup for session tree")
		} else {
			// No backup - use primary store directly
			logger.Warn().Msg("Using SimpleMap in-memory store without backup. Data will not be persisted on restart.")
			treeStore = primaryStore
		}

	case InMemoryPebbleStoreFilename:
		// Pebble in-memory storage (with in-memory VFS)
		logger.Warn().Msg("Using Pebble in-memory store. Data will not be persisted on restart.")
		pebbleStore, err := pebble.NewKVStore("") // Empty string triggers in-memory VFS in Pebble
		if err != nil {
			return nil, err
		}
		treeStore = pebbleStore

	default:
		// Treat anything else as disk-based persistent storage using Pebble
		// This would improve RAM/IO efficiency as KV databases are optimized for this pattern.
		storePath = filepath.Join(storesDirectoryPath, supplierOperatorAddress, sessionHeader.SessionId)

		// Make sure storePath does not exist when creating a new SessionTree
		if _, err := os.Stat(storePath); err != nil && !os.IsNotExist(err) {
			return nil, ErrSessionTreeStorePathExists.Wrapf("storePath: %q", storePath)
		}
		logger.Info().Msgf("Using %s as the store path for session tree", storePath)

		pebbleStore, err := pebble.NewKVStore(storePath)
		if err != nil {
			return nil, err
		}
		treeStore = pebbleStore
	}

	// Update the logger with the store path
	logger = logger.With("store_path", storePath)

	// Create the SMST from the KVStore and a nil value hasher so the proof would
	// contain a non-hashed Relay that could be used to validate the proof onchain.
	trie := smt.NewSparseMerkleSumTrie(treeStore, protocol.NewTrieHasher(), protocol.SMTValueHasher())

	// Create the sessionTree
	sessionTree := &sessionTree{
		logger:                  logger,
		sessionHeader:           sessionHeader,
		storePath:               storePath,
		treeStore:               treeStore,
		sessionSMT:              trie,
		sessionMu:               &sync.Mutex{},
		supplierOperatorAddress: supplierOperatorAddress,
	}

	return sessionTree, nil
}

// importSessionTree reconstructs a previously created session tree from its persisted state on disk.
// This function handles two distinct scenarios:
// 1. Importing a claimed session (claim != nil): The tree is in a read-only state with a fixed root hash
// 2. Importing an unclaimed session (claim == nil): The tree is in a mutable state and can accept updates
//
// Returns a fully reconstructed SessionTree or an error if reconstruction fails
func importSessionTree(
	logger polylog.Logger,
	sessionSMT *prooftypes.SessionSMT,
	claim *prooftypes.Claim,
	storesDirectoryPath string,
	backupConfig *BackupConfig, // Optional backup configuration
) (relayer.SessionTree, error) {
	sessionId := sessionSMT.SessionHeader.SessionId
	supplierOperatorAddress := sessionSMT.SupplierOperatorAddress
	applicationAddress := sessionSMT.SessionHeader.ApplicationAddress
	serviceId := sessionSMT.SessionHeader.ServiceId
	smtRoot := sessionSMT.SmtRoot

	// Determine the storage path based on configuration
	var storePath string

	isSimpleInMemoryStore := storesDirectoryPath == InMemoryStoreFilename
	isInMemoryPebbleStore := storesDirectoryPath == InMemoryPebbleStoreFilename
	isInMemoryStore := isSimpleInMemoryStore || isInMemoryPebbleStore
	isBackupEnabled := backupConfig != nil && backupConfig.Enabled
	isInMemoryWithBackup := isInMemoryStore && isBackupEnabled
	switch {
	case isInMemoryWithBackup:
		// In-memory mode with backup - use backup path
		storePath = backupConfig.GetBackupPath(supplierOperatorAddress, sessionId)
	case !isInMemoryStore:
		// Regular disk storage mode
		storePath = filepath.Join(storesDirectoryPath, supplierOperatorAddress, sessionId)
	default:
		// Pure in-memory mode without backup - cannot restore
		logger.Warn().
			Str("mode", storesDirectoryPath).
			Msg("Cannot import session tree in pure in-memory mode without backup")
		return nil, fmt.Errorf("cannot import session tree: in-memory mode without backup")
	}

	// Verify the storage path exists - if not, the session data is missing or corrupted
	if _, err := os.Stat(storePath); err != nil {
		logger.Error().Err(err).Msgf("session tree store path does not exist: %q", storePath)
		return nil, err
	}

	// Initialize the basic session tree structure with metadata
	sessionTree := &sessionTree{
		logger:                  logger,
		sessionHeader:           sessionSMT.SessionHeader,
		storePath:               storePath,
		sessionMu:               &sync.Mutex{},
		supplierOperatorAddress: supplierOperatorAddress,
	}

	logger = logger.With(
		"service_id", serviceId,
		"application_address", applicationAddress,
		"supplier_operator_address", supplierOperatorAddress,
		"session_id", sessionId,
	)

	// SCENARIO 1: Session has been claimed
	// When a claim exists, the session tree is ready to be processed by the proof submission step.
	// The tree storage is not loaded immediately as it will only be needed if proof generation is requested.
	if claim != nil {
		sessionTree.claimedRoot = claim.RootHash
		sessionTree.isClaiming = true
		logger.Info().Msg("imported a session tree WITH A PREVIOUSLY COMMITTED onchain claim")
		return sessionTree, nil
	}

	// SCENARIO 2: Session has not been claimed
	// The session is still active and mutable, so we need to reconstruct the full tree
	// from the persisted storage to allow for additional relay updates.

	var treeStore kvstore.MapStore

	if isInMemoryWithBackup {
		// In-memory mode with backup - restore from backup to in-memory store
		logger.Info().Msg("Restoring session tree from backup to in-memory store")

		// Create primary in-memory store
		primaryStore := simplemap.NewSimpleMap()

		// Open backup store to read data
		backupStore, err := pebble.NewKVStore(storePath)
		if err != nil {
			return nil, fmt.Errorf("failed to open backup store for restoration: %w", err)
		}

		// Create a temporary non-throttled backup wrapper for restoration
		// We'll restore from backup to primary, then wrap with throttling for future writes
		tempBackupWrapper := NewBackupKVStore(logger, primaryStore, backupStore)

		// Restore data from backup to primary store
		if err := tempBackupWrapper.RestoreFromBackup(); err != nil {
			return nil, fmt.Errorf("failed to restore from backup: %w", err)
		}

		// Now create the backup wrapper for ongoing operations
		treeStore = NewBackupKVStore(logger, primaryStore, backupStore)

		logger.Info().Msg("Successfully restored session tree from backup to in-memory store")
	} else {
		// Regular disk storage mode - open the existing KVStore
		var err error
		treeStore, err = pebble.NewKVStore(storePath)
		if err != nil {
			return nil, err
		}
	}

	// Reconstruct the SMST from the KVStore data using the previously saved root as the starting point
	trie := smt.ImportSparseMerkleSumTrie(treeStore, protocol.NewTrieHasher(), smtRoot, protocol.SMTValueHasher())

	sessionTree.sessionSMT = trie
	sessionTree.treeStore = treeStore
	sessionTree.claimedRoot = nil  // explicitly set for posterity
	sessionTree.isClaiming = false // explicitly set for posterity

	logger.Info().Msg("imported a session tree WITHOUT A PREVIOUSLY COMMITTED onchain claim")

	return sessionTree, nil
}

// GetSession returns the session corresponding to the SMST.
func (st *sessionTree) GetSessionHeader() *sessiontypes.SessionHeader {
	return st.sessionHeader
}

// Update is a wrapper for the SMST's Update function. It updates the SMST with
// the given key, value, and weight.
// This function should be called by the Miner when a Relay has been successfully served.
// It returns an error if the SMST has been flushed to disk which indicates
// that updates are no longer allowed.
func (st *sessionTree) Update(key, value []byte, weight uint64) error {
	st.sessionMu.Lock()
	defer st.sessionMu.Unlock()

	if st.claimedRoot != nil {
		return ErrSessionTreeClosed
	}

	err := st.sessionSMT.Update(key, value, weight)
	if err != nil {
		return ErrSessionUpdatingTree.Wrapf("error: %v", err)
	}

	// DO NOT DELETE: Uncomment this for debugging and change to .Debug logs post MainNet.
	// count := st.sessionSMT.MustCount()
	// sum := st.sessionSMT.MustSum()
	// st.logger.Debug().Msgf("session tree updated and has count %d and sum %d", count, sum)

	return nil
}

// ProveClosest is a wrapper for the SMST's ProveClosest function. It returns a proof for the given path.
// This function is intended to be called after a session has been claimed and needs to be proven.
// If the proof has already been generated, it returns the cached proof.
// It returns an error if the SMST has not been flushed yet (the claim has not been generated)
func (st *sessionTree) ProveClosest(path []byte) (compactProof *smt.SparseCompactMerkleClosestProof, err error) {
	st.sessionMu.Lock()
	defer st.sessionMu.Unlock()

	logger := st.logger.With("method", "ProveClosest")

	// A claim need to be generated before a proof can be generated.
	if st.claimedRoot == nil {
		return nil, ErrSessionTreeNotClosed
	}

	// If the proof has already been generated, return the cached proof.
	if st.compactProof != nil {
		// Make sure the path is the same as the one for which the proof was generated.
		if !bytes.Equal(path, st.proofPath) {
			return nil, ErrSessionTreeProofPathMismatch
		}

		return st.compactProof, nil
	}

	// Restore the sessionSMT from the persisted or in-memory storage.
	sessionSMT, err := st.restoreSessionSMT()
	if err != nil {
		return nil, err
	}
	if sessionSMT == nil {
		logger.Error().Msg("ðŸš¨ SHOULD RARELY HAPPEN: sessionSMT is NULL after restoration attempt! Cannot generate proof! Claims exist but REWARDS WILL BE LOST! ðŸš¨")
		return nil, fmt.Errorf("sessionSMT is nil - cannot generate proof for session %s", st.sessionHeader.SessionId)
	}

	// Generate the proof and cache it along with the path for which it was generated.
	// There is no ProveClosest variant that generates a compact proof directly.
	// Generate a regular SparseMerkleClosestProof then compact it.
	proof, err := sessionSMT.ProveClosest(path)
	if err != nil {
		logger.Error().Err(err).Msg("ðŸš¨ SHOULD RARELY HAPPEN: Proving the path in the sessionSMT failed! Cannot generate proof! Claims exist but REWARDS WILL BE LOST! ðŸš¨")
		return nil, err
	}

	compactProof, err = smt.CompactClosestProof(proof, sessionSMT.Spec())
	if err != nil {
		return nil, err
	}

	compactProofBz, err := compactProof.Marshal()
	if err != nil {
		return nil, err
	}

	// If no error occurred, cache the proof and the path for which it was generated.
	st.sessionSMT = sessionSMT
	st.proofPath = path
	st.compactProof = compactProof
	st.compactProofBz = compactProofBz

	return st.compactProof, nil
}

// restoreSessionSMT restores the sessionSMT from the persisted or in-memory storage.
func (st *sessionTree) restoreSessionSMT() (smt.SparseMerkleSumTrie, error) {
	logger := st.logger.With("method", "restoreSessionSMT")

	var sessionSMT smt.SparseMerkleSumTrie
	switch st.storePath {

	// Restoring from in-memory storage
	case InMemoryStoreFilename, InMemoryPebbleStoreFilename:
		// In memory storage: sessionSMT should still be available (preserved during Flush)
		if st.sessionSMT != nil {
			logger.Debug().Msgf("Found cached in memory sessionSMT for session %s. Restoring from in-memory store: %s", st.sessionHeader.SessionId, st.storePath)
			return st.sessionSMT, nil
		}
		// Fallback: reimport from the active SimpleMap store
		if st.treeStore != nil {
			logger.Debug().Msgf("Found cached in memory treeStore for session %s. Restoring from in-memory store: %s", st.sessionHeader.SessionId, st.storePath)
			sessionSMT = smt.ImportSparseMerkleSumTrie(st.treeStore, protocol.NewTrieHasher(), st.claimedRoot, protocol.SMTValueHasher())
			return sessionSMT, nil
		}
		// Should never happen
		return nil, fmt.Errorf("cannot restore sessionSMT from SimpleMap store for session %s", st.sessionHeader.SessionId)

	// Restoring from disk storage
	default:
		// Only supporting pebble storage for now
		pebbleStore, pebbleErr := pebble.NewKVStore(st.storePath)
		if pebbleErr != nil {
			logger.Error().Err(pebbleErr).Msgf("ðŸš¨ SHOULD RARELY HAPPEN: Failed to restore sessionSMT from disk store for session %s and store path %s", st.sessionHeader.SessionId, st.storePath)
			return nil, pebbleErr
		}
		st.treeStore = pebbleStore
		sessionSMT = smt.ImportSparseMerkleSumTrie(st.treeStore, protocol.NewTrieHasher(), st.claimedRoot, protocol.SMTValueHasher())
	}

	return sessionSMT, nil
}

// GetProofBz returns the marshaled proof for the session.
func (st *sessionTree) GetProofBz() []byte {
	return st.compactProofBz
}

// GetTrieSpec returns the trie spec of the SMST.
func (st *sessionTree) GetTrieSpec() smt.TrieSpec {
	return *st.sessionSMT.Spec()
}

// GetProof returns the proof for the SMST if it has been generated or nil otherwise.
func (st *sessionTree) GetProof() *smt.SparseCompactMerkleClosestProof {
	return st.compactProof
}

// Flush gets the root hash of the SMST needed for submitting the claim;
// then commits the entire tree to disk and stops the KVStore.
// It should be called before submitting the claim onchain. This function frees up the KVStore resources.
// If the SMST has already been flushed to disk, it returns the cached root hash.
func (st *sessionTree) Flush() (SMSTRoot []byte, err error) {
	st.sessionMu.Lock()
	defer st.sessionMu.Unlock()

	logger := st.logger.With("method", "Flush")

	// We already have the root hash, return it.
	if st.claimedRoot == nil {
		st.claimedRoot = st.sessionSMT.Root()
	}

	// Post-flush cleanup: handle different storage types appropriately
	switch st.storePath {
	case InMemoryStoreFilename:
		// SimpleMap with backup: Close backup store but keep primary in memory
		// TODO_IN_THIS_COMMIT: revisit this - don't like this type assertion...
		// consider setting the st.treeStore type to BackupKVStore...
		if backupStore, ok := st.treeStore.(*BackupKVStore); ok {
			// Close backup store to free file handles - primary remains in memory
			if err := backupStore.CloseBackupStore(); err != nil {
				logger.Warn().Err(err).Msg("failed to close backup store after flush")
			}
			logger.Debug().Msg("SimpleMap session tree flushed - backup closed, primary kept in memory for proof generation")
		} else {
			// Pure SimpleMap: Keep everything in memory for later proof generation
			logger.Debug().Msg("SimpleMap session tree flushed - keeping data in memory for proof generation")
		}
		// DEV_NOTE: DO NOT set either st.treeStore OR st.sessionSMT to nil here or proof generation will fail.

	case InMemoryPebbleStoreFilename:
		// Pebble in-memory: Stop the store but preserve the sessionSMT reference
		// We can't restore data from disk later, so the SMT reference is crucial
		if err := st.stop(); err != nil {
			return nil, err
		}
		st.treeStore = nil

		// DEV_NOTE: DO NOT set st.sessionSMT to nil here or proof generation will fail.
		// The following test will fail if we set st.sessionSMT to nil:
		// go test -count=1 -v ./pkg/relayer/session/... -run TestStorageModePebbleInMemory/TestOriginalBugReproduction
		// st.sessionSMT = nil
		logger.Debug().Msg("Pebble in-memory session tree stopped - sessionSMT preserved for proof generation")

	default:
		// Disk storage: Stop store and clear references (will be restored from disk for proofs)
		if err := st.stop(); err != nil {
			return nil, err
		}
		// DEV_NOTE: We can set both st.treeStore AND st.sessionSMT to nil here since we will be restoring from disk for proofs.
		st.treeStore = nil
		st.sessionSMT = nil
		logger.Debug().Msg("Disk session tree stopped - data will be restored from disk for proof generation")
	}

	return st.claimedRoot, nil
}

// GetSMSTRoot returns the root hash of the SMST.
// This function is used to get the root hash of the SMST at any time.
// In particular, it is used to get the root hash of the SMST before it is flushed to disk
// for use-cases like restarting the relayer and resuming ongoing sessions.
func (st *sessionTree) GetSMSTRoot() (smtRoot smt.MerkleSumRoot) {
	if st.sessionSMT == nil {
		return nil
	}
	return st.sessionSMT.Root()
}

// GetClaimRoot returns the root hash of the SMST needed for creating the claim.
// It returns the root hash of the SMST only if the SMST has been flushed to disk.
func (st *sessionTree) GetClaimRoot() []byte {
	return st.claimedRoot
}

// Delete deletes the SMST from the KVStore and removes the sessionTree from the RelayerSessionsManager.
// WARNING: This function deletes the KVStore associated to the session and should be
// called only after the proof has been successfully submitted onchain and the servicer
// has confirmed that it has been rewarded.
func (st *sessionTree) Delete() error {
	logger := st.logger.With("method", "Delete")

	st.sessionMu.Lock()
	defer st.sessionMu.Unlock()
	// After deletion, set treeStore to nil to:
	// - Prevent double-close operations.
	// - Avoid panics from future use of a closed kvstore instance.
	// - Signal that the treeStore is no longer valid.
	defer func() { st.treeStore = nil }()

	st.isClaiming = false

	// NB: We used to call `st.treeStore.ClearAll()` here.
	// This was intentionally removed to lower the IO load.
	// When the database is closed, it is deleted it from disk right away.

	// Handle stopping the KVStore if it's still active
	if st.treeStore == nil {
		logger.Info().Msg("KVStore is already stopped")
		return nil
	}

	switch st.storePath {

	case InMemoryStoreFilename:
		logger.Info().Msg("Clearing SimpleMap in-memory session tree KVStore.")
		if st.treeStore == nil {
			logger.Debug().Msg("SimpleMap treeStore is nil - nothing to clear")
			return nil
		}
		return st.treeStore.ClearAll()

	case InMemoryPebbleStoreFilename:
		logger.Info().Msg("Clearing Pebble in-memory session tree KVStore.")
		// Pebble in-memory stores need to be stopped
		if pebbleStore, ok := st.treeStore.(pebble.PebbleKVStore); ok {
			if err := pebbleStore.Stop(); err != nil {
				return err
			}
		}

	default:
		// Disk-based stores need to be stopped
		if pebbleStore, ok := st.treeStore.(pebble.PebbleKVStore); ok {
			if err := pebbleStore.Stop(); err != nil {
				return err
			}
		}
		return os.RemoveAll(st.storePath)
	}

	return nil
}

// StartClaiming marks the session tree as being picked up for claiming,
// so it won't be picked up by the relayer again.
// It returns an error if it has already been marked as such.
func (st *sessionTree) StartClaiming() error {
	st.sessionMu.Lock()
	defer st.sessionMu.Unlock()

	if st.isClaiming {
		return ErrSessionTreeAlreadyMarkedAsClaimed
	}

	st.isClaiming = true
	return nil
}

// GetSupplierOperatorAddress returns a stringified bech32 address of the supplier
// operator this sessionTree belongs to.
func (st *sessionTree) GetSupplierOperatorAddress() string {
	return st.supplierOperatorAddress
}

// stop the KVStore and free up the in-memory resources used by the session tree.
//
// Calling stop:
// - DOES NOT calculate the root hash of the SMST.
// - DOES commit the latest (current state) of the SMT to on-state storage.
func (st *sessionTree) stop() error {
	if st.treeStore == nil {
		return nil
	}

	// Commit any pending changes to the KVStore before stopping it.
	if err := st.sessionSMT.Commit(); err != nil {
		return err
	}

	// Handle stopping based on storage type
	switch st.storePath {
	case InMemoryStoreFilename:
		// SimpleMap in-memory storage: no stopping required
		return nil

	case InMemoryPebbleStoreFilename:
		// Pebble in-memory storage: call Stop() if it's a PebbleKVStore
		if pebbleStore, ok := st.treeStore.(pebble.PebbleKVStore); ok {
			return pebbleStore.Stop()
		}
		return nil

	default:
		// Disk-based persistent storage: call Stop() if it's a PebbleKVStore
		if pebbleStore, ok := st.treeStore.(pebble.PebbleKVStore); ok {
			return pebbleStore.Stop()
		}
		return nil
	}
}
